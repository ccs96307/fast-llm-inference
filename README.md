# Fast LLM Inference
## TODO List
- [x] 2024/11/08 | The implementation of `Speculative Decoding` followed [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192)
- [ ] 2024/11/15 | The implementation of `Self-Speculative Decoding` followed [Draft & Verify - Lossless Large Language Model Acceleration via Self-Speculative Decoding](https://arxiv.org/pdf/2309.08168)
- [ ] 2024/11/22 | The implementation of `Kangaroo` followed [Kangaroo - Lossless Self-Speculative Decoding via Double Early Exiting](https://arxiv.org/pdf/2404.18911)
- [ ] 2024/11/29 | The implementation of `Medusa` followed [Medusa - Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/pdf/2401.10774)
---
- [ ] ????/??/?? | The implementation of `prompt look-up decoding` followed [prompt-lookup-decoding](https://github.com/apoorvumang/prompt-lookup-decoding)
- [ ] ????/??/?? | The implementation of `UAG` followed [Universal Assisted Generation: Faster Decoding with Any Assistant Model](https://huggingface.co/blog/universal_assisted_generation)