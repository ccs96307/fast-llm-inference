{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers.models.llama.modeling_llama import Cache, DynamicCache, LlamaSdpaAttention\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KangarooModelMode:\n",
    "    draft_only_mode: str = \"draft_only\"\n",
    "    target_only_model: str = \"target_only\"\n",
    "    accelerate_mode: str = \"accelerate\"\n",
    "\n",
    "\n",
    "class KangarooLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.draft_mode_adapter_layer = LlamaSdpaAttention(config=config, layer_idx=config.num_hidden_layers)\n",
    "        self.shallow_layers = None\n",
    "        self.mode = KangarooModelMode.target_only_model\n",
    "    \n",
    "    def set_skip_layer(self, shallow_layer_num: int) -> None:\n",
    "        self.shallow_layers = self.model.layers[:shallow_layer_num]\n",
    "        self.remaining_layers = self.model.layers[shallow_layer_num:]\n",
    "\n",
    "    def set_draft_mode(self) -> None:\n",
    "        self.mode = KangarooModelMode.draft_only_mode\n",
    "\n",
    "    def set_target_mode(self) -> None:\n",
    "        self.mode = KangarooModelMode.target_only_model\n",
    "\n",
    "    def set_acceleration_mode(self) -> None:\n",
    "        self.mode = KangarooModelMode.accelerate_mode\n",
    "\n",
    "    def get_shallow_layer_output(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **loss_kwargs,\n",
    "    ) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **loss_kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        if self.mode == KangarooModelMode.target_only_model:\n",
    "            return super(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                cache_position=cache_position,\n",
    "                num_logits_to_keep=num_logits_to_keep,\n",
    "                **loss_kwargs,\n",
    "            )\n",
    "        elif self.mode == KangarooModelMode.draft_only_mode:\n",
    "            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "            output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "            ### === MODEL ===\n",
    "            if self.model.gradient_checkpointing and self.model.training and use_cache:\n",
    "                use_cache = False\n",
    "\n",
    "            if inputs_embeds is None:\n",
    "                inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "\n",
    "            # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "            return_legacy_cache = False\n",
    "            if use_cache and not isinstance(past_key_values, Cache):\n",
    "                return_legacy_cache = True\n",
    "                if past_key_values is None:\n",
    "                    past_key_values = DynamicCache()\n",
    "                else:\n",
    "                    past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "\n",
    "            if cache_position is None:\n",
    "                past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "                cache_position = torch.arange(\n",
    "                    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "                )\n",
    "            if position_ids is None:\n",
    "                position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "            causal_mask = self.model._update_causal_mask(\n",
    "                attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "            )\n",
    "            hidden_states = inputs_embeds\n",
    "\n",
    "            # create position embeddings to be shared across the decoder layers\n",
    "            position_embeddings = self.model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "            # decoder layers\n",
    "            all_hidden_states = () if output_hidden_states else None\n",
    "            all_self_attns = () if output_attentions else None\n",
    "            next_decoder_cache = None\n",
    "\n",
    "            for decoder_layer in self.shallow_layers:\n",
    "                if output_hidden_states:\n",
    "                    all_hidden_states += (hidden_states,)\n",
    "\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "                if use_cache:\n",
    "                    next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "                if output_attentions:\n",
    "                    all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            ### Adapter\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.norm(hidden_states)\n",
    "\n",
    "            # add hidden states from the last decoder layer\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            next_cache = next_decoder_cache if use_cache else None\n",
    "            if return_legacy_cache:\n",
    "                next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "            hidden_states, self_attn_weights, past_key_values = self.draft_mode_adapter_layer(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.norm(hidden_states)\n",
    "\n",
    "            if self.config.pretraining_tp > 1:\n",
    "                lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "                logits = [torch.nn.functional.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "                logits = torch.cat(logits, dim=-1)\n",
    "            else:\n",
    "                # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "                logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)\n",
    "\n",
    "            return CausalLMOutputWithPast(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                past_key_values=past_key_values,\n",
    "                hidden_states=hidden_states,\n",
    "                attentions=self_attn_weights,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of KangarooLlamaForCausalLM were not initialized from the model checkpoint at ../../models/HuggingFaceTB--SmolLM2-135M-Instruct and are newly initialized: ['draft_mode_adapter_layer.k_proj.weight', 'draft_mode_adapter_layer.o_proj.weight', 'draft_mode_adapter_layer.q_proj.weight', 'draft_mode_adapter_layer.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = KangarooLlamaForCausalLM.from_pretrained(\"../../models/HuggingFaceTB--SmolLM2-135M-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
