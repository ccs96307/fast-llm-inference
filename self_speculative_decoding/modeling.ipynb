{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
    "# and OPT implementations in this library. It has been modified from its\n",
    "# original forms to accommodate minor architectural differences compared\n",
    "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    GenerationMixin,\n",
    ")\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaMLP,\n",
    "    LlamaRMSNorm,\n",
    "    LlamaRotaryEmbedding,\n",
    "    LLAMA_ATTENTION_CLASSES,\n",
    ")\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSkipLlamaDecoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: LlamaConfig,\n",
    "        layer_idx: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set skip layer\n",
    "        skip_layer_ids = {\"attn\": [], \"mlp\": []}\n",
    "        self.draft_mode = False\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "        self.layer_idx = layer_idx\n",
    "    \n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def set_skip_layer_ids(self, skip_layer_ids: Dict[str, List[int]]):\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "\n",
    "    def set_draft_mode(self, _mode: bool):\n",
    "        self.draft_mode = _mode\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        if self.draft_mode and self.layer_idx in self.skip_layer_ids[\"attn\"]:\n",
    "            hidden_states = residual\n",
    "            self_attn_weights = None\n",
    "            present_key_value = None\n",
    "        else:\n",
    "            hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "            # Self Attention\n",
    "            hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                **kwargs,\n",
    "            )\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "\n",
    "        if self.draft_mode and self.layer_idx in self.skip_layer_ids[\"mlp\"]:\n",
    "            hidden_states = residual\n",
    "        else:\n",
    "            hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "            hidden_states = self.mlp(hidden_states)\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSkipLlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Set skip layer\n",
    "        skip_layer_ids = {\"attn\": [], \"mlp\": []}\n",
    "        self.draft_mode = False\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = torch.nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [LayerSkipLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def set_skip_layer_ids(self, skip_layer_ids: Dict[str, List[int]]):\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.set_skip_layer_ids(skip_layer_ids=skip_layer_ids)\n",
    "\n",
    "    def set_draft_mode(self, _mode: bool):\n",
    "        self.draft_mode = _mode\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.set_draft_mode(_mode=_mode)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        if use_cache and not isinstance(past_key_values, Cache):\n",
    "            return_legacy_cache = True\n",
    "            if past_key_values is None:\n",
    "                past_key_values = DynamicCache()\n",
    "            else:\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                    position_embeddings,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n",
    "        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n",
    "        # to infer the attention mask.\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        using_static_cache = isinstance(past_key_values, StaticCache)\n",
    "\n",
    "        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n",
    "        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n",
    "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
    "                attention_mask,\n",
    "                inputs_embeds=input_tensor,\n",
    "                past_key_values_length=past_seen_tokens,\n",
    "                is_training=self.training,\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if using_static_cache:\n",
    "            target_length = past_key_values.get_max_cache_shape()\n",
    "        else:\n",
    "            target_length = (\n",
    "                attention_mask.shape[-1]\n",
    "                if isinstance(attention_mask, torch.Tensor)\n",
    "                else past_seen_tokens + sequence_length + 1\n",
    "            )\n",
    "\n",
    "        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n",
    "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=sequence_length,\n",
    "            target_length=target_length,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and attention_mask is not None\n",
    "            and attention_mask.device.type == \"cuda\"\n",
    "            and not output_attentions\n",
    "        ):\n",
    "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
    "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
    "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
    "\n",
    "        return causal_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype,\n",
    "        device: torch.device,\n",
    "        cache_position: torch.Tensor,\n",
    "        batch_size: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            )\n",
    "            if sequence_length != 1:\n",
    "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                    padding_mask, min_dtype\n",
    "                )\n",
    "\n",
    "        return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSkipLlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Set skip layer\n",
    "        skip_layer_ids = {\"attn\": [], \"mlp\": []}\n",
    "        self.draft_mode = False\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "\n",
    "        self.model = LayerSkipLlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def set_skip_layer_ids(self, skip_layer_ids: Dict[str, List[int]]):\n",
    "        assert \"attn\" in skip_layer_ids and \"mlp\" in skip_layer_ids, \"`skip_layer_ids` need to be set `attn` and `mlp`!\"\n",
    "        assert isinstance(skip_layer_ids[\"attn\"], list), \"`skip_layer_ids['attn']` need to be a list!\"\n",
    "        assert isinstance(skip_layer_ids[\"mlp\"], list), \"`skip_layer_ids['mlp']` need to be a list!\"\n",
    "\n",
    "        for attn_layer_idx in skip_layer_ids[\"attn\"]:\n",
    "            assert attn_layer_idx < len(self.model.layers), f\"attn_layer_idx {attn_layer_idx} is out of Range ({len(self.model.layers)})\" \n",
    "            \n",
    "        for mlp_layer_idx in skip_layer_ids[\"mlp\"]:\n",
    "            assert mlp_layer_idx < len(self.model.layers), f\"mlp_layer_idx {mlp_layer_idx} is out of Range ({len(self.model.layers)})\"\n",
    "\n",
    "        self.skip_layer_ids = skip_layer_ids\n",
    "        self.model.set_skip_layer_ids(skip_layer_ids=skip_layer_ids)\n",
    "\n",
    "        print(\"skip_layer_ids:\", self.skip_layer_ids)\n",
    "\n",
    "    def set_draft_mode(self, _mode: bool):\n",
    "        self.draft_mode = _mode\n",
    "        self.model.set_draft_mode(_mode=_mode)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **loss_kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from layerskip_modeling.modeling_layerskip_llama import LayerSkipLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"../models/HuggingFaceTB--SmolLM2-1.7B-Instruct/\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = LayerSkipLlamaForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attn': [], 'mlp': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.skip_layer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_layer_ids: {'attn': [2, 15, 18], 'mlp': [2, 15, 18]}\n"
     ]
    }
   ],
   "source": [
    "skip_layer_ids = {\n",
    "    \"attn\": [\n",
    "        2,\n",
    "        15,\n",
    "        18,\n",
    "    ],\n",
    "    \"mlp\": [\n",
    "        2,\n",
    "        15,\n",
    "        18,\n",
    "    ]\n",
    "}\n",
    "\n",
    "model.set_skip_layer_ids(skip_layer_ids=skip_layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Taiwan. And why?\",\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_token_num = inputs[\"input_ids\"].shape[1]\n",
    "prompt_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 146])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of Taiwan. And why?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of Taiwan is Taipei. It is the largest city in Taiwan and serves as the political, economic, and cultural center of the country. The reason for this is that Taipei was established as the capital city in 1949, following the Chinese Civil War, when the government of the Republic of China (ROC) relocated from mainland China to Taiwan. This decision was made to ensure the continuity of the ROC's political and administrative functions, and to maintain its claim to the entirety of China.<|im_end|>\n",
      "\n",
      "Completion Token Number: 110\n",
      "Cost Time: 1.9064702987670898, Speed: 57.698250044145325 token/sec\n"
     ]
    }
   ],
   "source": [
    "skip_layer_ids = {\n",
    "    \"attn\": [\n",
    "        2,\n",
    "        15,\n",
    "        18,\n",
    "    ],\n",
    "    \"mlp\": [\n",
    "        2,\n",
    "        15,\n",
    "        18,\n",
    "    ]\n",
    "}\n",
    "\n",
    "model.set_skip_layer_ids(skip_layer_ids=skip_layer_ids)\n",
    "\n",
    "\n",
    "model.set_draft_mode(False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "total_token_num = outputs[0].shape[0]\n",
    "completion_token_num = total_token_num - prompt_token_num\n",
    "cost_time = time.time() - start_time\n",
    "\n",
    "token_per_second = completion_token_num / cost_time\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(response)\n",
    "print()\n",
    "print(f\"Completion Token Number: {completion_token_num}\")\n",
    "print(f\"Cost Time: {cost_time}, Speed: {token_per_second} token/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 146])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of Taiwan. And why?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of Taiwan is Taipei.<|im_end|>\n",
      "\n",
      "Completion Token Number: 13\n",
      "Cost Time: 0.2020103931427002, Speed: 64.3531245979844 token/sec\n"
     ]
    }
   ],
   "source": [
    "model.set_draft_mode(True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "total_token_num = outputs.shape[-1]\n",
    "completion_token_num = total_token_num - prompt_token_num\n",
    "cost_time = time.time() - start_time\n",
    "\n",
    "token_per_second = completion_token_num / cost_time\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(response)\n",
    "print()\n",
    "print(f\"Completion Token Number: {completion_token_num}\")\n",
    "print(f\"Cost Time: {cost_time}, Speed: {token_per_second} token/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
